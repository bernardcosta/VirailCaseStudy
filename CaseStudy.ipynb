{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgi\n",
    "import requests\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "import csv\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "### 1. Data Retrieval\n",
    "According to IMDB documentation, the datasets are updated once a day. Therefore a batch download of the dataset once a day is sufficient.\n",
    "This can be done with a background script that is running the below code in a server. (ex: use *cron* or python *schedule* library) \n",
    "\n",
    "The code is designed to treat the links to the datasets as APIs. I.e. a GET request to each dataset is called on a daily basis. This assumes a certain ammount of reliability form IMDB. \n",
    "\n",
    "*(The alternative is to scrape the website itself which is trivial with the current state of the site but is definitely not as robust as the above approach)*\n",
    "\n",
    "Key points:\n",
    " -  The data is kept as compressed for deep storage and not overwritten. This is useful for version controls especially since IMDB does not offer this.\n",
    " - If failure to connect to IMDB, the below code retires 5 times with a time delay.\n",
    " - If attempts have been exhausted, a notification should be sent as a fallback for engineers to resolve any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['https://datasets.imdbws.com/name.basics.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.akas.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.basics.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.crew.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.episode.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.principals.tsv.gz',\n",
    "            'https://datasets.imdbws.com/title.ratings.tsv.gz']\n",
    "\n",
    "mappings = ['mappings/name_basics_mappings.json',\n",
    "            'mappings/title_akas_mapping.json', \n",
    "            'mappings/title_basics_mapping.json',\n",
    "            'mappings/title_crew_mappings.json',\n",
    "            'mappings/title_eposides_mappings.json',\n",
    "            'mappings/title_principals_mapping.json',\n",
    "            'mappings/title_ratings_mappings.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block should be run once a day in an automated process\n",
    "attempts = 5\n",
    "for attempt in range(attempts):\n",
    "    try:\n",
    "        # Create subdirectory to save today's data in\n",
    "        timestr = time.strftime(\"%Y%m%d\")\n",
    "        if not os.path.exists(timestr):\n",
    "            os.makedirs(timestr)\n",
    "    \n",
    "        # Download data files to storage\n",
    "        for link in datasets:\n",
    "            file = requests.get(link)\n",
    "            # Save to directory\n",
    "            with open(os.path.join(timestr, link.split(\"/\")[-1]), \"wb\") as zip:\n",
    "                zip.write(file.content)\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise e\n",
    "        # wait 2 minutes before retry\n",
    "        time.sleep(120)\n",
    "        \n",
    "        if attempt == attempts - 1:\n",
    "            # Notify admins/engineers that data retrieval for the day was skipped.\n",
    "            pass\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    print(\"success\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning and Building indices\n",
    "\n",
    "Compressed data in a server is not useful to anyone. The next step in the data pipeline is to make the data easily accessible both for analytics and for product integration purposes. The below code can also be run once a day after step 1.\n",
    "\n",
    "#### A. Cleaning data\n",
    "\n",
    "The IMDB data contains `\\N` for null value. For elasticsearch this is problematic since in order for ES to recognise null values they have to be in the same mapping type of their corresponding field. For example: `\\N` in the `startYear` field can be turned to -1 (integer that will not be used by any record).\n",
    "\n",
    "\n",
    "#### B. Building indices\n",
    "\n",
    " - The size of each dataset should be noted in order to set the number of shards if needed (for performance).\n",
    " - In a clustes its a good idea to use replicas (faster search and robust against down time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"20201119\"\n",
    "es = Elasticsearch(host='localhost', port=9200)\n",
    "\n",
    "# Create or update index for each dataset\n",
    "for i, dataset in enumerate(datasets):\n",
    "    name = dataset.split(\"/\")[-1]\n",
    "    index_name = name.split(\".\")[0] + \"_\" + name.split(\".\")[1]\n",
    "    # Load corresponding mapping and create index if not already created\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name, body=json.load(open(mappings[0])))\n",
    "    # unzip raw file and bulk index\n",
    "    with gzip.open(os.path.join(directory, name), \"rt\", encoding='UTF-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        helpers.bulk(es, index=index_name, actions=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
